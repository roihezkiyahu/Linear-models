---
title: "EXAM"
author: "roi hezkiyahu"
date: "7 6 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Q1

![](Q1.png)

### I

$$
Y_{ij} \sim N(\beta^tx_i,\sigma^2) \Rightarrow \bar{Y_i} \sim(\beta^tx_i,\frac{\sigma^2}{m}) \textrm{ as a sum of normal distributions}
\\
\textrm{Define } Z^t = (\bar{Y_1},\bar{Y_2},\ldots,\bar{Y_n}) ;\bar{Y_i},\bar{Y_j} \textrm{ are i.i.d } \forall i\ne j
\\
\textrm{thus: } Z \sim N(X\beta,\frac{\sigma^2}{m}I)
\\
\textrm{the MLE for this case is:}
\\
\hat{\beta}_{MLE} = (X^tX)^{-1}X^tZ
\\
\hat{\frac{\sigma^2_{MLE}}{m}} = \frac{RSS}{n} \Rightarrow \sigma^2_{MLE} = \frac{m}{n}RSS 
\\
\textrm{where } RSS = (Z-X\hat{\beta}_{MLE})^t(Z-X\hat{\beta}_{MLE})
$$

### II

$$
E(\hat{\beta}_{MLE}) = E((X^tX)^{-1}X^tZ) = (X^tX)^{-1}X^tE(Z) = (X^tX)^{-1}X^tX\beta = \beta \Rightarrow \hat{\beta}_{MLE} \textrm{ is unbiased}
\\
\textrm {we know that } \frac{mRSS}{\sigma^2} \sim \chi^2_{n-p} \Rightarrow E(RSS) = \frac{(n-p)\sigma^2}{m}
\\
E(\hat{\sigma^2}_{MLE}) = \frac{m}{n}E(RSS) = \frac{m}{n}E(RSS) = \frac{(n-p)\sigma^2}{n} \Rightarrow \hat{\sigma^2}_{MLE} \textrm{ is biased}
\\
\textrm{thus if we take } \hat{S^2} = \frac{m}{n-p}RSS \textrm{ we get an unbiased estimator}
\\
E(S^2) = \frac{m}{n-p}E(RSS) = \frac{(n-p)\sigma^2}{n-p} = \sigma^2
$$
### III

$$
\textrm{we calculated the expected value of } \hat{\beta} \textrm{ lets calculate the variance, and keep in mind that it distribute normally as a linear transofrmation of a normal variable}
\\
V(\hat{\beta}) = V((X^tX)^{-1}X^tZ) = (X^tX)^{-1}X^tV(Z)X(X^tX)^{-1} = V(Z)(X^tX)^{-1}
\\
\hat{\beta} \sim N(\beta,\frac{\sigma^2}{m}(X^tX)^{-1})
\\
\textrm{therefor an exact } 100(1-\alpha) \textrm {% confidence interval is:}
\\
(\hat{\beta}- Z_{1-\frac{\alpha}{2}}\sqrt{\frac{\sigma^2}{m}(X^tX)^{-1}},\hat{\beta} + Z_{1-\frac{\alpha}{2}}\sqrt{\frac{\sigma^2}{m}(X^tX)^{-1}})
\\
\textrm{for a bonferroni adjustment all we nee to do is divide } \alpha \textrm{ by p and derive the following: }
\\
(\hat{\beta}- Z_{1-\frac{\alpha}{2p}}\sqrt{\frac{\sigma^2}{m}(X^tX)^{-1}},\hat{\beta} + Z_{1-\frac{\alpha}{2p}}\sqrt{\frac{\sigma^2}{m}(X^tX)^{-1}})
$$

# Q2

![](Q2.png)

### I.1

$$
A = (I_m,0_{m\times p-m})
\\
C = \underline 0 \in \mathbb R^{m}
\\
A\beta = (\beta_1,\ldots,\beta_m) = \underline 0
$$

### I.2

$$
\textrm{Define: }
\\
1^t = (1,\ldots,1) \in \mathbb R^{m-1}
\\
D = diag(-1)\in \mathbb R^{{m-1}\times{m-1}}
\\
A = (\underline 1,D,0_{m\times p-m})\in \mathbb R^{{m-1}\times{p}}
\\
C = \underline 0 \in \mathbb R^{m}
\\
A\beta = C \Rightarrow(A\beta)_{i} = \beta_1 - \beta_{i+1} = 0 \Rightarrow \beta_1 = \beta_{i+1} \ \forall \ 0<i<m \Rightarrow \beta_1 = \ldots = \beta_m
\\
$$

### I.3

$$
A = (0,1,0,\ldots,0)
\\
C = 6
\\
A\beta = C \Rightarrow \beta_2 = 6
$$

### I.4

$$
A = \begin{bmatrix}
    1 & 0 & \ldots &\ldots &\ldots &\ldots & 0
    \\ 0 & -3 & 1 & 0  & \ldots & \ldots & 0
    \\ 0 & 0 & 1 & -1 & -2 & \ldots & 0
  \end{bmatrix}
\\
C = (7,-4,0)^t
\\
A\beta = (\beta_1,-3\beta_2 + \beta_3,\beta_3-\beta_4-2\beta_5)^t = C \Rightarrow 
\\
\beta_1 = 7
\\
-3\beta_2 + \beta_3 = -4 \Rightarrow \beta_3 =3\beta_2 -4
\\
\beta_3 -\beta_4-2\beta_5 = 0 \Rightarrow 2\beta_5 = \beta_3 -\beta_4
$$


### II

$$
\textrm{in OLS we have the following optimization problem: }
\\
\textrm{minimize } f:= (Y-X\beta)^t(Y-X\beta)
\\
s.t \ A\beta =C
\\
\textrm {denote the argmin of this optimization problem as }  \beta^c
\\
\textrm{with lagrange multipliers we get the equivalent problem of: }
\\
\textrm{minimize } f:= (Y-X\beta)^t(Y-X\beta) - \lambda (A\beta -C)
\\
\frac{\partial f}{\partial \beta} = 2 X^tX\beta -3X^tY -2 A^t \lambda^t := 0 \Rightarrow \hat{\beta^c} = (X^tX)^{-1}(X^tY+A^t\lambda^t)
\\
A\beta = C \Rightarrow A(X^tX)^{-1}X^tY+A(X^tX)^{-1}A^t\lambda^t=C \Rightarrow A(X^tX)^{-1}A^t\lambda^t = C-A\hat{\beta_{OLS}}
\\
\textrm{thus we get: }
\\
\lambda^t = A(X^tX)^{-1}A^t)^{-1}(C-A\hat{\beta_{OLS}})
\\
\textrm{plug it back in and we get:}
\\
\hat{\beta^c} = (X^tX)^{-1}(X^tY+A^t(A(X^tX)^{-1}A^t)^{-1}(C-A\hat{\beta_{OLS}}))) = \hat{\beta_{OLS}} + (X^tX)^{-1}A^t(A(X^tX)^{-1}A^t)^{-1}(C-A\hat{\beta_{OLS}})
$$

### III

$$
\textrm{under } H_0:
\\
E(A\hat{\beta}_{OLS}) = A\beta = C
\\
V(A\hat{\beta}_{OLS}) = AV(\hat{\beta}_{OLS})A^t = A(X^tX)^{-1}A^t\sigma^2
\\
\textrm{we know that } \hat{\beta}_{OLS} \sim N(\beta,(X^tX)^{-1}\sigma^2)
\\
\textrm{thus } A\hat{\beta}_{OLS} \sim N(C,A(X^tX)^{-1}A^t\sigma^2) \textrm{ as a linear transformation of a normal random variable}
\\
\textrm{also we know that: } (A\hat{\beta}_{OLS}- C)^t(A(X^tX)^{-1}A^t\sigma^2)^{-1}(A\hat{\beta}_{OLS}- C)  ~\chi^2_p
\\
\textrm{and also we know that: } \frac{RSS}{\sigma^2} \sim \chi^2_{n-p}
\\
\textrm{thus we can derive the following F-test:}
\\
\frac{(A\hat{\beta}_{OLS}- C)^t(A(X^tX)^{-1}A^t\sigma^2)^{-1}(A\hat{\beta}_{OLS}- C) }{\frac{RSS}{\sigma^2}}\frac{n-p}{p} = \frac{(A\hat{\beta}_{OLS}- C)^t(A(X^tX)^{-1}A^t)^{-1}(A\hat{\beta}_{OLS}- C) }{RSS}\frac{n-p}{p} \sim F_{p,n-p}
$$

# Q3

![](Q3.png)

# Q4

![](Q4.png)

# Q5

![](Q5.png)

# Story

![](S1.png)

![](S2.png)

![](S3.png)

![](S4.png)

![](S5.png)

![](S6.png)

![](S7.png)
